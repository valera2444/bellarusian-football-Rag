{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "ollama run gemma2:2b\n",
    "\n",
    "!pip install ollama\n",
    "\n",
    "!pip install -U langchain-ollama\n",
    "\n",
    "pip install langchain-huggingface ~5 min\n",
    "\n",
    "!pip install --upgrade --quiet  langchain langchain-community langchain-openai langchain-experimental neo4j\n",
    "\n",
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valeriy/python_projects/RAG/RAG_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cuda:0'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/txts/cleaned.txt'\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"belfoot_collection\",\n",
    "    embedding_function=hf_embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "    \n",
    "\n",
    "documents = [Document (page_content=text, id=idx) for idx, text in enumerate(lines)]\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "#vector_store.add_documents(documents=documents, ids=uuids) #CPU - >1.5 hours, GPU - 15min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"gemma2:2b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K_SPARSE = 7\n",
    "TOP_K_DENSE = 7\n",
    "NUMBER_OF_ATTEMPTS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm_25_retriever = BM25Retriever.from_documents(documents, k=TOP_K_SPARSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    system: str\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    attempts: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Literal\n",
    "def should_continue(state: State) -> Literal[\"YES\", \"NO\"]:\n",
    "    \"\"\"Complex routing logic\"\"\"\n",
    "\n",
    "    if state['attempts'] >= NUMBER_OF_ATTEMPTS:\n",
    "        return \"end\"\n",
    "    \n",
    "    answer_stripped = state['answer'].strip()\n",
    "    if answer_stripped == \"YES\":\n",
    "        return \"end\"\n",
    "    elif answer_stripped == \"NO\":\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        print(answer_stripped)\n",
    "        assert False\n",
    "\n",
    "def redirect_reject(state: State) -> Literal[\"retrival\", \"generation\"]:\n",
    "    \"\"\"Complex routing logic\"\"\"\n",
    "\n",
    "    answer_stripped = state['answer'].strip()\n",
    "    if answer_stripped == \"RETRIVAL\":\n",
    "        return \"retrival\"\n",
    "    elif answer_stripped == \"GENERATION\":\n",
    "        return \"generation\"\n",
    "    else:\n",
    "        print(answer_stripped)\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "from langchain.schema import Document\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "def rewrite_query(state: State):\n",
    "    system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "    You have access to a database of telegram posts about Belarusian football for building LLM-powered applications. \\\n",
    "\n",
    "    Perform query decomposition. Given a user question, break it down into distinct sub questions that \\\n",
    "    you need to answer in order to answer the original question.\n",
    "\n",
    "    If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
    "    \n",
    "    Do not add any additional information or explanation\"\"\"\n",
    "    \n",
    "    messages = {'system': system,\n",
    "                'question': state['question'],\n",
    "                'answer':''}\n",
    "\n",
    "    resp = model.invoke(str(messages))\n",
    "\n",
    "    return {\"question\":resp}\n",
    "\n",
    "def retrieve(state: State):\n",
    "    bm25_docs = bm_25_retriever.invoke(state[\"question\"])\n",
    "    vector_docs = vector_store.similarity_search(state[\"question\"], k=TOP_K_DENSE)\n",
    "    \n",
    "    # Merge and remove duplicates\n",
    "    doc_dict = {doc.page_content: doc for doc in bm25_docs + vector_docs}\n",
    "    combined_docs = list(doc_dict.values())\n",
    "\n",
    "    return {\"context\": combined_docs, \"attempts\": 0}\n",
    "\n",
    "def generate(state: State):\n",
    "    \n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    return {\"answer\": response, 'attempts':state['attempts']+1}\n",
    "\n",
    "def verify_answer(state: State):\n",
    "    verification_prompt = {\n",
    "        \"question\": state[\"question\"],\n",
    "        \"context\": \"\\n\\n\".join(doc.page_content for doc in state[\"context\"]),\n",
    "        \"answer\": state[\"answer\"]\n",
    "    }\n",
    "    verification_result = model.invoke(f\"How do you think, does your answer answers the question correctly and completely? Double-check alignment with context in order not to mislead. Respond with 'YES' or 'NO'. \\\n",
    "                                       \\n\\n{verification_prompt} \\\n",
    "                                       \\n\\nSingle word: YES or NO\")\n",
    "\n",
    "    return {\"answer\":verification_result}\n",
    "\n",
    "def fault_reason(state: State):\n",
    "    verification_prompt = {\n",
    "        \"question\": state[\"question\"],\n",
    "        \"context\": \"\\n\\n\".join(doc.page_content for doc in state[\"context\"]),\n",
    "        \"answer\": state[\"answer\"]\n",
    "    }\n",
    "    verification_result = model.invoke(f\"Why do you think answer is not good enough?.\\n\\n{verification_prompt} \\\n",
    "                                       \\n\\nRespond with single word : RETRIVAL if you think that retrived data is not enough; and GENERATION if retrived data is good enough and problem is in generation stage\")\n",
    "\n",
    "    return {\"answer\":verification_result}\n",
    "\n",
    "# Build the LangGraph workflow\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"retrieve\", retrieve)\n",
    "graph_builder.add_node(\"generate\", generate)\n",
    "graph_builder.add_node(\"verify_answer\", verify_answer)\n",
    "graph_builder.add_node(\"fault_reason\", fault_reason)\n",
    "graph_builder.add_node(\"rewrite_query\", rewrite_query)\n",
    "\n",
    "# Define edges (flow control)\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph_builder.add_edge(\"retrieve\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", \"verify_answer\")\n",
    "graph_builder.add_edge(\"rewrite_query\", \"retrieve\")\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"verify_answer\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"continue\": \"fault_reason\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"fault_reason\",\n",
    "    redirect_reject,\n",
    "    {\n",
    "        \"generation\": 'generate',\n",
    "        \"retrival\": \"rewrite_query\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Best player of Belarus in 2025: \"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I cannot provide you with a complete list of every player in the history of the Neman club. \\n\\nHere\\'s why:\\n\\n* **Data availability:** Comprehensive historical data for all basketball clubs is not readily available online. This includes roster information, especially for smaller or amateur teams like Neman.  \\n* **Privacy and data protection:**  Sharing personal details of past players without their consent would be a breach of privacy. \\n\\n\\n**Where you might find some information:**\\n\\n* **Neman website:** Check the club\\'s official website (if they have one) for historical information, perhaps under sections like \"History,\" \"About us,\" or \"Club Archives.\"\\n* **Basketball-related databases:** Some online basketball databases exist (like Eurobasket.com or FIBA.basketball), which may list some teams and their history, but it\\'s unlikely to include every single player of all time at a club like Neman. \\n\\nI hope this helps! \\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Please, list all Neman players of all time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
