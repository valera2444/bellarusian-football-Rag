{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_from_html(html_content):\n",
    "    \"\"\"\n",
    "    Extracts and returns only the text content from an HTML document.\n",
    "    :param html_content: A string containing the HTML content\n",
    "    :return: A string with extracted text\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "    # Remove script and style elements\n",
    "    for script_or_style in soup([\"script\", \"style\"]):\n",
    "        script_or_style.decompose()\n",
    "    \n",
    "    # Extract text from <div class=\"text\">\n",
    "    text_divs = [div.get_text(separator=' ', strip=True) for div in soup.find_all(\"div\", class_=\"text\")]\n",
    "    \n",
    "    return text_divs\n",
    "\n",
    "def process_html_files(folder_path, output_json_path):\n",
    "    \"\"\"\n",
    "    Process all HTML files in a folder and save extracted text into a JSON file.\n",
    "    :param folder_path: Path to the folder containing HTML files\n",
    "    :param output_json_path: Path to save the output JSON file\n",
    "    \"\"\"\n",
    "    extracted_data = {}\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".html\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                html_content = file.read()\n",
    "                extracted_data[filename] = extract_text_from_html(html_content)\n",
    "    \n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(extracted_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"data/row_data\" # Change this to the folder containing HTML files\n",
    "output_json_path = \"data/extracted_text.json\" # Change this to the desired output JSON file\n",
    "process_html_files(folder_path, output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_json_into_txt(json_path, txt_folder, num_parts=7):\n",
    "    \"\"\"\n",
    "    Split a JSON file into multiple TXT files with equal sizes.\n",
    "    :param json_path: Path to the JSON file\n",
    "    :param txt_folder: Folder to save the TXT files\n",
    "    :param num_parts: Number of TXT files to create\n",
    "    \"\"\"\n",
    "    if not os.path.exists(txt_folder):\n",
    "        os.makedirs(txt_folder)\n",
    "    \n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "        all_text = []\n",
    "        \n",
    "        for key, value in data.items():\n",
    "            all_text.append(f\"{key}:\\n\")\n",
    "            if isinstance(value, list):\n",
    "                all_text.extend(value)\n",
    "            else:\n",
    "                all_text.append(str(value))\n",
    "            all_text.append(\"\\n\\n\")\n",
    "        \n",
    "    with open(os.path.join(txt_folder, f\"all.txt\"), \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(\"\\n\".join(all_text))\n",
    "            \n",
    "    total_size = len(all_text)\n",
    "    chunk_size = total_size // num_parts\n",
    "\n",
    "    if num_parts == 1:\n",
    "        txt_path = os.path.join(txt_folder, f\"all_russian.txt\")\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(\"\\n\".join(all_text))\n",
    "        return\n",
    "\n",
    "    for i in range(num_parts):\n",
    "        start_index = i * chunk_size\n",
    "        end_index = (i + 1) * chunk_size if i < num_parts - 1 else total_size\n",
    "\n",
    "        txt_path = os.path.join(txt_folder, f\"part_{i+1}.txt\")\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(\"\\n\".join(all_text[start_index:end_index]))\n",
    "\n",
    "# Example usage\n",
    "json_path = \"data/extracted_text.json\"  # Change to actual JSON file path\n",
    "txt_folder = \"data/txts\"  # Change to output folder path\n",
    "split_json_into_txt(json_path, txt_folder,num_parts=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "all_russian.txt -> all_translated.txt\n",
    "\n",
    "With yabdex preferably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "translates_txt = 'data/txts/all_translated.txt'\n",
    "with open(translates_txt, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "    f = txt_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30768"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.count('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/txts/emoji_free.txt', 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(remove_emojis(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Check data/txts/cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"data/txts/emoji_free.txt\"   # Change this to your file name\n",
    "output_file = \"data/txts/cleaned.txt\" # Change if needed\n",
    "\n",
    "# Read the file and process lines\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Remove leading spaces and \"?\" signs\n",
    "processed_lines = [re.sub(r\"^[\\s?]+\", \"\", line) for line in lines]\n",
    "\n",
    "# Write the processed lines to a new file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(processed_lines)\n",
    "\n",
    "print(\"Processing complete. Check\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/txts/cleaned.txt', 'r', encoding=\"utf-8\") as file:\n",
    "    f = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30609"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.count('\\n')\n",
    "#Was 30768....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bakhar occurence: 3\n",
      "Bahar occurence: 198\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print('Bakhar occurence:', len([m.start() for m in re.finditer('Bakhar', f)]))\n",
    "print('Bahar occurence:',len([m.start() for m in re.finditer('Bahar', f)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
